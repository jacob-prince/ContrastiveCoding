## A Contrastive Coding Account of Category Selectivity in the Ventral Visual Stream

***Jacob S. Prince, George A. Alvarez, and Talia Konkle***  
*Harvard University, Dept. of Psychology*

![twitter_5](https://github.com/jacob-prince/PROJECT_DNFFA/assets/35503086/0ae3bcd0-b368-4677-ac4e-e9335e2e5412)

This repository houses code for generating figures in the following bioRxiv preprint: **link here** 
All plotting routines depend on helper functions that exist in the following utilities repository: https://github.com/jacob-prince/jsputils

For questions, please feel free to email me: jacob.samuel.prince@gmail.com.

---------------

**Abstract:**

Modular and distributed theories of category selectivity along the ventral visual stream have long existed in tension. Here, we present a reconciling framework, based on a series of analyses relating category-selective tuning within biological and artificial neural networks. We discover that, in models trained with contrastive self-supervised objectives over a rich natural image diet, visual category-selective tuning naturally emerges for classic categories of faces, bodies, scenes, and words. Further, lesions of these model units lead to selective, dissociable recognition deficits. Finally, these pre-identified units from a single model can predict neural responses in all corresponding face-, scene-, body-, and word-selective regions of the human visual system, even under a constrained sparse-positive encoding procedure. The success of this model indicates that the nature of category-selective tuning in the human brain (e.g. for faces or scenes) is dependent on the learning diet, and best understood in the context of the full scope of experienced visual input. Broadly, we offer a unifying theoretical account where category-selective tuning naturally emerges as a consequence of positive information routing through hierarchical population codes, in order to disentangle the statistics of visual experience.

