{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training CIFAR-10 in 75 seconds on a single GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following in command line to create a conda environment with FFCV installed:\n",
    "\n",
    "# conda create -n ffcv python=3.9 cupy pkg-config compilers libjpeg-turbo \\\n",
    "# opencv pytorch torchvision cudatoolkit=11.3 numba -c conda-forge -c pytorch \\\n",
    "# && conda activate ffcv && conda update ffmpeg && pip install ffcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "\n",
    "from ffcv.fields import IntField, RGBImageField\n",
    "from ffcv.fields.decoders import IntDecoder, SimpleRGBImageDecoder\n",
    "from ffcv.loader import Loader, OrderOption\n",
    "from ffcv.pipeline.operation import Operation\n",
    "from ffcv.transforms import RandomHorizontalFlip, Cutout, \\\n",
    "    RandomTranslate, Convert, ToDevice, ToTensor, ToTorchImage\n",
    "from ffcv.transforms.common import Squeeze\n",
    "from ffcv.writer import DatasetWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create an FFCV-compatible CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_dir = '/home/jovyan/work/DataLocal-w/ffcv-cifar10/'\n",
    "\n",
    "datasets = {\n",
    "    'train': torchvision.datasets.CIFAR10(cifar_dir, train=True, download=True),\n",
    "    'test': torchvision.datasets.CIFAR10(cifar_dir, train=False, download=True)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (name, ds) in datasets.items():\n",
    "    writer = DatasetWriter(f'{cifar_dir}/cifar_{name}.beton', {\n",
    "        'image': RGBImageField(),\n",
    "        'label': IntField()\n",
    "    })\n",
    "    writer.from_indexed_dataset(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that statistics are wrt to uin8 range, [0,255].\n",
    "CIFAR_MEAN = [125.307, 122.961, 113.8575]\n",
    "CIFAR_STD = [51.5865, 50.847, 51.255]\n",
    "device = 'cuda:1'\n",
    "num_workers = 128\n",
    "\n",
    "BATCH_SIZE = 400\n",
    "\n",
    "loaders = {}\n",
    "for name in ['train', 'test']:\n",
    "    label_pipeline: List[Operation] = [IntDecoder(), ToTensor(), ToDevice(device), Squeeze()]\n",
    "    image_pipeline: List[Operation] = [SimpleRGBImageDecoder()]\n",
    "\n",
    "    # Add image transforms and normalization\n",
    "    if name == 'train':\n",
    "        image_pipeline.extend([\n",
    "            RandomHorizontalFlip(),\n",
    "            RandomTranslate(padding=2),\n",
    "            Cutout(8, tuple(map(int, CIFAR_MEAN))), # Note Cutout is done before normalization.\n",
    "        ])\n",
    "    image_pipeline.extend([\n",
    "        ToTensor(),\n",
    "        ToDevice(device, non_blocking=True),\n",
    "        ToTorchImage(),\n",
    "        Convert(torch.float16),\n",
    "        torchvision.transforms.Normalize(CIFAR_MEAN, CIFAR_STD),\n",
    "    ])\n",
    "\n",
    "    # Create loaders\n",
    "    loaders[name] = Loader(f'{cifar_dir}/cifar_{name}.beton',\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            num_workers=num_workers,\n",
    "                            order=OrderOption.RANDOM,\n",
    "                            drop_last=(name == 'train'),\n",
    "                            pipelines={'image': image_pipeline,\n",
    "                                       'label': label_pipeline})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Setup model architecture and optimization parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mul(nn.Module):\n",
    "    def __init__(self, weight):\n",
    "        super(Mul, self).__init__()\n",
    "        self.weight = weight\n",
    "    def forward(self, x): return x * self.weight\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x): return x.view(x.size(0), -1)\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super(Residual, self).__init__()\n",
    "        self.module = module\n",
    "    def forward(self, x): return x + self.module(x)\n",
    "\n",
    "def conv_bn(channels_in, channels_out, kernel_size=3, stride=1, padding=1, groups=1):\n",
    "    return nn.Sequential(\n",
    "            nn.Conv2d(channels_in, channels_out,\n",
    "                         kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                         groups=groups, bias=False),\n",
    "            nn.BatchNorm2d(channels_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "model = nn.Sequential(\n",
    "    conv_bn(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "    conv_bn(64, 128, kernel_size=5, stride=2, padding=2),\n",
    "    Residual(nn.Sequential(conv_bn(128, 128), conv_bn(128, 128))),\n",
    "    conv_bn(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "    nn.MaxPool2d(2),\n",
    "    Residual(nn.Sequential(conv_bn(256, 256), conv_bn(256, 256))),\n",
    "    conv_bn(256, 128, kernel_size=3, stride=1, padding=0),\n",
    "    nn.AdaptiveMaxPool2d((1, 1)),\n",
    "    Flatten(),\n",
    "    nn.Linear(128, NUM_CLASSES, bias=False),\n",
    "    Mul(0.2)\n",
    ")\n",
    "model = model.to(device,memory_format=torch.channels_last)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD, lr_scheduler\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "opt = SGD(model.parameters(), lr=.5, momentum=0.9, weight_decay=5e-4)\n",
    "iters_per_epoch = 50000 // BATCH_SIZE\n",
    "lr_schedule = np.interp(np.arange((EPOCHS+1) * iters_per_epoch),\n",
    "                        [0, 5 * iters_per_epoch, EPOCHS * iters_per_epoch],\n",
    "                        [0, 1, 0])\n",
    "scheduler = lr_scheduler.LambdaLR(opt, lr_schedule.__getitem__)\n",
    "scaler = GradScaler()\n",
    "loss_fn = CrossEntropyLoss(label_smoothing=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastprogress import progress_bar\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for ep in progress_bar(range(EPOCHS)):\n",
    "    model.train()\n",
    "    for ims, labs in loaders['train']:\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        with autocast():\n",
    "            out = model(ims)\n",
    "            loss = loss_fn(out, labs)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_correct, total_num = 0., 0.\n",
    "        for ims, labs in loaders['test']:\n",
    "            with autocast():\n",
    "                out = (model(ims) + model(torch.fliplr(ims))) / 2. # Test-time augmentation\n",
    "                total_correct += out.argmax(1).eq(labs).sum().cpu().item()\n",
    "                total_num += ims.shape[0]\n",
    "\n",
    "        print(f'Epoch {ep} â€“ Accuracy: {total_correct / total_num * 100:.1f}%. Time elapsed: {round(time.time() - start,4)} sec')\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepnet-ffa",
   "language": "python",
   "name": "deepnet-ffa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
